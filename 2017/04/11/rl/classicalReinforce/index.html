<!DOCTYPE html>


  <html class="light page-post">


<head>
  <meta charset="utf-8">
  
  <title>Classical Reinforce Algorithm | Liu Xianggen</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="RL,reinforce," />
  

  <meta name="description" content="Bref[2]Classical REINFORCE [1] algorithm, also known as the “vanilla” policy gradient. We will start with an implementation that works with a fixed policy and environment. The next section Implementin">
<meta name="keywords" content="RL,reinforce">
<meta property="og:type" content="article">
<meta property="og:title" content="Classical Reinforce Algorithm">
<meta property="og:url" content="http://www.liuxianggen.com/2017/04/11/rl/classicalReinforce/index.html">
<meta property="og:site_name" content="Liu Xianggen">
<meta property="og:description" content="Bref[2]Classical REINFORCE [1] algorithm, also known as the “vanilla” policy gradient. We will start with an implementation that works with a fixed policy and environment. The next section Implementin">
<meta property="og:updated_time" content="2017-07-12T12:51:42.195Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Classical Reinforce Algorithm">
<meta name="twitter:description" content="Bref[2]Classical REINFORCE [1] algorithm, also known as the “vanilla” policy gradient. We will start with an implementation that works with a fixed policy and environment. The next section Implementin">

  

  
    <link rel="icon" href="/images/favicon.jpg">
  

  <link href="/css/styles.css?v=028c63b1" rel="stylesheet">


  

  

  <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?fb0e88cc5bbe470f7877739f0bf6bc4c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




</head>

<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Bref-2"><span class="toc-text">Bref[2]</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Preliminaries"><span class="toc-text">Preliminaries</span></a></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-rl/classicalReinforce" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Classical Reinforce Algorithm</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2017.04.11</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>LiuXianggen</span>
        </span>
      

      


      

    </div>
  </header>

  <div class="article-content">
    
      <h1 id="Bref-2"><a href="#Bref-2" class="headerlink" title="Bref[2]"></a>Bref[2]</h1><p>Classical REINFORCE [1] algorithm, also known as the “vanilla” policy gradient. We will start with an implementation that works with a fixed policy and environment. The next section Implementing New Algorithms (Advanced) will improve upon this implementation, utilizing the functionalities provided by the framework to make it more structured and command-line friendly.</p>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>First, let’s briefly review the algorithm along with some notations. We work with an MDP defined by the tuple $(S,A,P,r,\mu_0,\gamma,T)$, where $S$ is a set of states, $A$ is a set of actions, $P:S\times A\times S\rightarrow[0,1] $ is the transition probability, $r:S\times A \rightarrow R $ is the reward function, $\mu_0:S \rightarrow [0,1]$ is the initial state distribution, $\gamma\in[0,1] $ is the discount factor, and $ T \in N $ is the horizon. REINFORCE directly optimizes a parameterized stochastic policy $\pi_{\theta}:S\times A→[0,1]$ by performing gradient ascent on the expected return objective:</p>
<p>\begin{align}<br>\eta(\theta) &amp;= \mathbb{E}\left[\sum_{t=0}^T \gamma^t r(s_t, a_t)\right] \\<br>&amp;= \sum_HP(H)(\sum_{t=0}^T \gamma^t r(s_t, a_t)) \\<br>&amp;= \sum_Hp(s_0)\prod_{t=0}^{T-1}P(a_t|s_t)P(s_{t+1}|s_t,a_{t})(\sum_{t=0}^T \gamma^t r(s_t, a_t))<br>\end{align}<br>where the expectation is implicitly taken over all possible trajectories, following the sampling procedure $s_0\sim \mu_0$, $a_t\sim \pi_{\theta}(a_t|s_t)=P(a_t|s_t)$, and $s_{t+1} \sim P(s_{t+1}|s_t,a_t)$, and $H$ indicates a single possible trajectories.<br>Basicly,the return $r(s,a)$ and the transition probability$P(s_{t+1}|s_t,a_t)$ are both controlled by environment,and only $\pi_{theta}(a_t|s_t)$ is the function of $\theta$.<br>By the likelihood ratio trick, the gradient of the objective with respect to $\theta$ is given by the followings.<br>The upper formular may be frastrating, we first have,<br>\begin{align}<br>\nabla_{\theta}\prod\pi_{\theta}(a_t|s_t) &amp;= \prod\pi_{\theta}(a_t|s_t) \nabla_{\theta}log\prod\pi_{\theta}(a_t|s_t) \\<br>&amp;= \prod\pi_{\theta}(a_t|s_t) \sum_t\nabla_{\theta}log\pi_{\theta}(a_t|s_t)<br>\end{align}<br>and,<br>\begin{align}<br>\nabla_\theta \eta(\theta) &amp;= \sum_H  P(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\nabla_{\theta}\prod_{t=0}^{T}P(a_t|s_t)  (\sum_{t=0}^T \gamma^t r(s_t, a_t)) \\<br>&amp;= \sum_H P(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\nabla_{\theta}\prod_{t=0}^{T}\pi_{\theta}(a_t|s_t) (\sum_{t=0}^T \gamma^t r(s_t, a_t))\\<br>&amp;= \sum_H P(H) \sum_t\nabla log\pi_{\theta}(a_t|s_t)(\sum_{t=0}^T \gamma^t r(s_t, a_t))\\<br>&amp;=  \mathbb{E}\left[\left(\sum_{t=0}^T \gamma^t r(s_t, a_t)\right) \left(\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \right)\right]<br>\end{align}<br>Then we can reduce the variance of this estimator by noting that for $t′&lt;t$,<br>\begin{align}<br>E(t’) &amp;= \mathbb{E}\left[ r(s_{t’}, a_{t’}) \nabla_\theta \log \pi_\theta(a_t | s_t) \right] \\<br>&amp;= \sum_H P(H)\left[ r(s_{t’}, a_{t’}) \nabla_\theta \log \pi_\theta(a_t | s_t) \right] \\<br>&amp;= \sum_{H} P(H-a_t) \cdot P(a_t|s_t) r(s_{t’}, a_{t’}) \nabla_\theta \log \pi_\theta(a_t | s_t) \\<br>&amp;= \sum_{H} P(H-a_t) \cdot P(a_t|s_t) r(s_{t’}, a_{t’}) \nabla_\theta  \pi_\theta(a_t | s_t) \\<br>&amp;= \sum_{H-a_t} P(H-a_t) \cdot P(a_t|s_t) r(s_{t’}, a_{t’}) \nabla_\theta  \sum_{a_t}\pi_\theta(a_t | s_t) \\<br>&amp;= \sum_{H-a_t} P(H-a_t) \cdot P(a_t|s_t) r(s_{t’}, a_{t’}) \nabla_\theta 1 \\<br>&amp;= 0<br>\end{align}<br>Hence,<br>$$\nabla_\theta \eta(\theta) = \mathbb{E}\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t’=t}^T \gamma^{t’} r(s_{t’}, a_{t’}) \right]$$</p>
<p>Often, we use the following estimator instead:<br>$$\nabla_\theta \eta(\theta) = \mathbb{E}\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t’=t}^T \gamma^{t’-t} r(s_{t’}, a_{t’}) \right]$$<br>where $\gamma^{t’}$ is replaced by $\gamma^{t’-t}$. When viewing the discount factor as a variance reduction factor for the undiscounted objective, this alternative gradient estimator has less bias, at the expense of having a larger variance. We define $R_t := \sum_{t’=t}^T \gamma^{t’-t} r(s_{t’}, a_{t’})$as the empirical discounted return.</p>
<p>The above formula will be the central object of our implementation. The pseudocode for the whole algorithm is as below:<br>$\quad \cdot$ Initialize policy $\pi_{\theta}$ with parameter $\theta_0$.<br>$\quad \cdot$ For iteration $k=1,2,\dots$:<br>$\quad\quad \cdot$ Sample N trajectories $\tau_1,\dots,\tau_n$ under the current policy $\theta_k$, where $\tau_i = (s_t^i, a_t^i, R_t^i)_{t=0}^{T-1}$. Note that the last state is dropped since no action is taken after observing the last state.<br>$\quad\quad \cdot$ Compute the empirical policy gradient:<br>$$\widehat{\nabla_\theta \eta(\theta)} = \frac{1}{NT} \sum_{i=1}^N \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^i | s_t^i) R_t^i$$<br>$\quad\quad \cdot$ Take a gradient step: $\theta_{k+1} = \theta_k + \alpha \widehat{\nabla_\theta \eta(\theta)}$</p>
<p>[1]Williams, Ronald J. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” Machine learning 8.3-4 (1992): 229-256.<br>[2] this is a note for <a href="https://rllab.readthedocs.io/en/latest/user/implement\_algo\_basic.html?highlight=basic#id3" target="_blank" rel="external">rllab document</a><br>[3]<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html" target="_blank" rel="external">David Silver</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/21725498" target="_blank" rel="external">深度增强学习之Policy Gradient方法</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/22339097" target="_blank" rel="external">智能单元</a><br>[6]<a href="http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf" target="_blank" rel="external">deep rl tutorial</a></p>

    
  </div>
</article>

</div>


  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">挣点熬夜的咖啡钱</div>
        <ul class="theme.donation.items.length">
        
          <li class="item">
            <img src="/images/wechat_pay.jpg" alt="">
          </li>
        
          <li class="item">
            <img src="/images/alipay.jpg" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>




  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    




  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>
