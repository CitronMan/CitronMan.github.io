<!DOCTYPE html>


  <html class="light page-post">


<head>
  <meta charset="utf-8">
  
  <title>Probabilistic Graphical Model | LemonMan</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="study," />
  

  <meta name="description" content="Basis分布伯努利分布（Bernoulli Distribution）Bernoulli Distribution是最简单的二项式分布，伯努利试验是只有两种可能结果的单次随机试验，即对于一个随机变量Y而言：$$Y \sim Bernoulli(p)$$则概率函数、期望和方差分别是：$$ P(Y=y) = p \text{ or } P(y) = p^y(1-p)^{1-y} \\E[y]=p \">
<meta name="keywords" content="study">
<meta property="og:type" content="article">
<meta property="og:title" content="Probabilistic Graphical Model">
<meta property="og:url" content="http://lemonman.net/2016/09/23/ml/ProbabilisticGraphicalModel/index.html">
<meta property="og:site_name" content="LemonMan">
<meta property="og:description" content="Basis分布伯努利分布（Bernoulli Distribution）Bernoulli Distribution是最简单的二项式分布，伯努利试验是只有两种可能结果的单次随机试验，即对于一个随机变量Y而言：$$Y \sim Bernoulli(p)$$则概率函数、期望和方差分别是：$$ P(Y=y) = p \text{ or } P(y) = p^y(1-p)^{1-y} \\E[y]=p \">
<meta property="og:image" content="http://lemonman.net/images/study/CPDS.png">
<meta property="og:updated_time" content="2017-07-07T06:34:38.216Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Probabilistic Graphical Model">
<meta name="twitter:description" content="Basis分布伯努利分布（Bernoulli Distribution）Bernoulli Distribution是最简单的二项式分布，伯努利试验是只有两种可能结果的单次随机试验，即对于一个随机变量Y而言：$$Y \sim Bernoulli(p)$$则概率函数、期望和方差分别是：$$ P(Y=y) = p \text{ or } P(y) = p^y(1-p)^{1-y} \\E[y]=p \">
<meta name="twitter:image" content="http://lemonman.net/images/study/CPDS.png">

  

  
    <link rel="icon" href="/images/favicon.jpg">
  

  <link href="/css/styles.css?v=028c63b1" rel="stylesheet">


  

  

  
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?fb0e88cc5bbe470f7877739f0bf6bc4c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




</head>

<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Basis"><span class="toc-text">Basis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#分布"><span class="toc-text">分布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#伯努利分布（Bernoulli-Distribution）"><span class="toc-text">伯努利分布（Bernoulli Distribution）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二项分布-Binomial"><span class="toc-text">二项分布(Binomial)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#n个独立同分布的Bernoulli分布之和就是二项分布是B-n-p"><span class="toc-text">n个独立同分布的Bernoulli分布之和就是二项分布是B(n,p)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正态分布（Normal-Distribution）OR-高斯分布（Gaussian-distribution）"><span class="toc-text">正态分布（Normal Distribution）OR 高斯分布（Gaussian distribution）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#混合正态分布（multivariate-normal-distribution）"><span class="toc-text">混合正态分布（multivariate normal distribution）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Means-algorithm"><span class="toc-text">K-Means algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mixed-Gaussion-model"><span class="toc-text">Mixed Gaussion model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Expectation-Maximation"><span class="toc-text">Expectation Maximation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Jensen’s-Inequality"><span class="toc-text">Jensen’s Inequality</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lt-gt-x-E-x"><span class="toc-text"><==> x = E(x)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#factor-analysis"><span class="toc-text">factor analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#latent-variable-model"><span class="toc-text">latent variable model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Gaussian-Distribution"><span class="toc-text">The Gaussian Distribution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multivariate-Guassian-Distribtion"><span class="toc-text">Multivariate Guassian Distribtion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gaussian-Mixture-Distribution"><span class="toc-text">Gaussian Mixture Distribution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tree-based-Models"><span class="toc-text">Tree-based Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-text">LDA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PGM"><span class="toc-text">PGM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-CPDS"><span class="toc-text">Logistic CPDS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-odds"><span class="toc-text">Log-odds:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-change-of-log-odds-of-binary-variables"><span class="toc-text">The change of log odds of binary variables</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Factorization-Theorem-is-the-fundamental-for-representing-probability-using-directed-acyclic-graphical-models-BN-It-presents-the-sufficient-and-necessary-condition-between-the-graphical-topological-structures-and-the-conditional-independences"><span class="toc-text">Factorization Theorem is the fundamental for representing probability using directed acyclic graphical models(BN).It presents the sufficient and necessary condition between the graphical topological structures and the conditional independences.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#induced-markov-network-process"><span class="toc-text">induced markov network process:</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-ml/ProbabilisticGraphicalModel" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Probabilistic Graphical Model</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2016.09.23</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>LiuXianggen</span>
        </span>
      

      


      

    </div>
  </header>

  <div class="article-content">
    
      <h1 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h1><h2 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h2><h3 id="伯努利分布（Bernoulli-Distribution）"><a href="#伯努利分布（Bernoulli-Distribution）" class="headerlink" title="伯努利分布（Bernoulli Distribution）"></a>伯努利分布（Bernoulli Distribution）</h3><p>Bernoulli Distribution是最简单的二项式分布，伯努利试验是只有两种可能结果的单次随机试验，即对于一个随机变量Y而言：$$Y \sim Bernoulli(p)$$<br>则概率函数、期望和方差分别是：<br>$$ P(Y=y) = p \text{ or } P(y) = p^y(1-p)^{1-y} \\<br>E[y]=p \times 1+(1-p) \times 0 = p \\<br>Var[y]=p\times (1-E[y])^2 + (1-p)\times (0-E[y])^2 = p(1-p)$$</p>
<h3 id="二项分布-Binomial"><a href="#二项分布-Binomial" class="headerlink" title="二项分布(Binomial)"></a>二项分布(Binomial)</h3><p>二项分布(Binomial distribution)是n重伯努利试验成功次数的离散概率分布。<br>如果试验E是一个n重伯努利试验，每次伯努利试验的成功概率为p，X代表成功的次数，则X的概率分布是二项分布，记为$$X \sim B(n,p) \text{  OR} \\X \sim Binomial(n,p)$$其概率密度函数为<br>$$P(X = k) =C_n^k p^k(1-p)^{n-k}$$</p>
<h4 id="n个独立同分布的Bernoulli分布之和就是二项分布是B-n-p"><a href="#n个独立同分布的Bernoulli分布之和就是二项分布是B-n-p" class="headerlink" title="n个独立同分布的Bernoulli分布之和就是二项分布是B(n,p)"></a>n个独立同分布的Bernoulli分布之和就是二项分布是B(n,p)</h4><p>同朴素贝叶斯一样，高斯判别分析（Gaussian discriminant analysismodel, GDA）也是一种生成学习算法，在该模型中，我们假设y给定的情况下，x服从混合正态分布。通过训练确定参数，新样本通过已建立的模型计算出隶属不同类的概率，选取概率最大为样本所属的类。主要难点在于确定参数的过程。</p>
<h3 id="正态分布（Normal-Distribution）OR-高斯分布（Gaussian-distribution）"><a href="#正态分布（Normal-Distribution）OR-高斯分布（Gaussian-distribution）" class="headerlink" title="正态分布（Normal Distribution）OR 高斯分布（Gaussian distribution）"></a>正态分布（Normal Distribution）OR 高斯分布（Gaussian distribution）</h3><p>若随机变量$X$服从一个数学期望为$\mu$、标准方差为$\sigma^2$的高斯分布，记为：<br>$$X \sim \mathcal{N}(\mu,\sigma^2)$$其概率密度函数为<br>$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-u)^2}{2\sigma^2}}$$</p>
<h3 id="混合正态分布（multivariate-normal-distribution）"><a href="#混合正态分布（multivariate-normal-distribution）" class="headerlink" title="混合正态分布（multivariate normal distribution）"></a>混合正态分布（multivariate normal distribution）</h3><p>混合正态分布也称混合高斯分布。该分布的期望和协方差为多元的：期望$\mu \in R^n$,协方差$\Sigma \in R^{n\times n}$，协方差具有对称性和正定性。概率密度函数为<br>$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$其中，$\mu$为混合高斯分布的期望$E(X)$,$\Sigma$为其协方差$Cov(X)$，$|\Sigma|$表示协方差的行列式。<br>$   $Cov(X)=E[(X-E(X))(X-E(X))^T]$$</p>
<h2 id="K-Means-algorithm"><a href="#K-Means-algorithm" class="headerlink" title="K-Means algorithm"></a>K-Means algorithm</h2><p>K-Means不保证收敛到全局最优，可以通过多次初始化聚类中心然后选取最好的J来改进。<br>Density estimation<br>建立一个模型P(x)<br>{X_1,X_2,X_3…..,X_n}<br>X_k====&gt; P(X_k)=</p>
<h2 id="Mixed-Gaussion-model"><a href="#Mixed-Gaussion-model" class="headerlink" title="Mixed Gaussion model"></a>Mixed Gaussion model</h2><p>有一组数据<br>latent</p>
<h2 id="Expectation-Maximation"><a href="#Expectation-Maximation" class="headerlink" title="Expectation Maximation"></a>Expectation Maximation</h2><p>have some model for P(x,z,\theta)<br>observe only x.<br>try to maximize<br>$$ l(\theta) = \<br>$$</p>
<h2 id="Jensen’s-Inequality"><a href="#Jensen’s-Inequality" class="headerlink" title="Jensen’s Inequality"></a>Jensen’s Inequality</h2><p>let f be a convention function (e.g. f’’(x)&gt;0)<br>let X be a random  variable<br>then  f(E(x))&lt;= E(f(x))<br>Further if f’’(x)&gt;0 =&gt; f is a strictly concave<br>then<br>E[f(x)]=f(E(x))</p>
<h2 id="lt-gt-x-E-x"><a href="#lt-gt-x-E-x" class="headerlink" title="&lt;==&gt; x = E(x)"></a>&lt;==&gt; x = E(x)</h2><p>if f’’&lt;=0 ….</p>
<h3 id="factor-analysis"><a href="#factor-analysis" class="headerlink" title="factor analysis"></a>factor analysis</h3><p>There are some latent vatiable</p>
<p>$$<br>z \sim N(0,I), z \in R^d  (d&lt;n)<br>x|z \sim N(\mu+UP, \Phi) \\<br>where<br>$$</p>
<h2 id="latent-variable-model"><a href="#latent-variable-model" class="headerlink" title="latent variable model"></a>latent variable model</h2><p>In latent variable modelling the assumption is that the observed high-dimensional data is generated from an underlying low-dimensional process.[1]</p>
<ol>
<li>Two factors X and Y will independently cause Z<br>$$\\ \Rightarrow P(XYZ)=P(X)P(Y)P(Z|X,Y) $$<br>NOT: $ P(XYZ)=P(Z)P(Y|Z)P(X|Z)$<br>$$P(X_1 X_2 … X_N)=\prod_{i=1}^{i=N}P(X_i|X_1 … X_{i-1})$$</li>
</ol>
<ol>
<li>Terms<br><strong>Larger odds</strong>: mean the configuration is more likely to cause the positive result. </li>
</ol>
<ol>
<li>二项分布即n次伯努利实验<br>正如Beta分布是二项式分布的共轭先验概率分布，狄利克雷分布作为多项式分布的共轭先验概率分布。<h2 id="The-Gaussian-Distribution"><a href="#The-Gaussian-Distribution" class="headerlink" title="The Gaussian Distribution"></a>The Gaussian Distribution</h2>The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable $x$, the Gaussian distribution can be written in the form<br>$$\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$<br>where $\mu$ is the mean and $\sigma^2$ is the variance.<h2 id="Multivariate-Guassian-Distribtion"><a href="#Multivariate-Guassian-Distribtion" class="headerlink" title="Multivariate Guassian Distribtion"></a>Multivariate Guassian Distribtion</h2>For a $D$-dimensional vector $x$, the multivariate Gaussian distribution takes the form<br>$$\mathcal{N} (\mathbf{x}|\mathbf{\mu},\Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{(|\Sigma|)^{1/2}}e^{-\frac{1}{2}(\mathbf{x-\mu})^T\Sigma^{-1}(\mathbf{x-\mu})}$$<br>where $\mu$ is a $D$-dimensional mean vector, $\Sigma$ is a $D\times D$ covariance matrix, and $|\Sigma|$ denotes the determinant of $\Sigma$.<br><strong>Survival analysis</strong> is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems.<h2 id="Gaussian-Mixture-Distribution"><a href="#Gaussian-Mixture-Distribution" class="headerlink" title="Gaussian Mixture Distribution"></a>Gaussian Mixture Distribution</h2>It can be written as a linear superposition of Gaussians in the form:<br>$$p(x)=\sum_{k=1}^K \pi_kN(x|\mu_k,{\Sigma}_k)$$<h2 id="Tree-based-Models"><a href="#Tree-based-Models" class="headerlink" title="Tree-based Models"></a>Tree-based Models</h2>There are various simple, but widely used, models that work by partitioning the input space into cuboid regions, whose edges are aligned with the axes, and then assigning a simple model (for example, a constant) to each region. They can be viewed as a model combination method in which only one model is responsible for making predictions at any given point in input space. The process of selecting a specific model, given a new input x, can be described by a sequential decision making process corresponding to the traversal of a binary tree (one that splits into two branches at each node). Here we focus on a particular tree-based framework called classification and regression trees, or CART (Breiman et al., 1984), although there are many other variants going by such names as ID3 and C4.5.[1]</li>
</ol>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>第一个是用于自然语言分析的隐主题模型。LDA是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。文档到主题服从Dirichlet分布，主题到词服从多项式分布。<br>       第二个线性判别式分析（Linear Discriminant Analysis），简称为LDA。也称为Fisher线性判别（Fisher Linear Discriminant，FLD），是模式识别的经典算法，在1996年由Belhumeur引入模式识别和人工智能领域。<br>基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。</p>
<h1 id="PGM"><a href="#PGM" class="headerlink" title="PGM"></a>PGM</h1><h2 id="Logistic-CPDS"><a href="#Logistic-CPDS" class="headerlink" title="Logistic CPDS"></a>Logistic CPDS</h2><p>Logistic CPDS在计算神经学中也有应用，将在<a href="/2016/09/16/Synapse-Model">Synapse Model博客</a>中详细叙述。</p>
<div align="center"><br><img src="/images/study/CPDS.png" width="250" align="center"><br></div>

<h3 id="Log-odds"><a href="#Log-odds" class="headerlink" title="Log-odds:"></a>Log-odds:</h3><p>$$O=\frac{P(Y=y^1|X_1,…,x_K)}{P(Y=y^0|X_1,…,x_K)}=e^Z$$</p>
<h3 id="The-change-of-log-odds-of-binary-variables"><a href="#The-change-of-log-odds-of-binary-variables" class="headerlink" title="The change of log odds of binary variables"></a>The change of log odds of binary variables</h3><p>$$\bigtriangleup O = \frac{O(X_{-j},X_j=x_j^1)}{O(X_{-j},X_j=x_j^0)}=e^{w_j}$$<br>$w_j&gt;0$means positive contribution</p>
<h2 id="Factorization-Theorem-is-the-fundamental-for-representing-probability-using-directed-acyclic-graphical-models-BN-It-presents-the-sufficient-and-necessary-condition-between-the-graphical-topological-structures-and-the-conditional-independences"><a href="#Factorization-Theorem-is-the-fundamental-for-representing-probability-using-directed-acyclic-graphical-models-BN-It-presents-the-sufficient-and-necessary-condition-between-the-graphical-topological-structures-and-the-conditional-independences" class="headerlink" title="Factorization Theorem is the fundamental for representing probability using directed acyclic graphical models(BN).It presents the sufficient and necessary condition between the graphical topological structures and the conditional independences."></a>Factorization Theorem is the fundamental for representing probability using directed acyclic graphical models(BN).It presents the sufficient and necessary condition between the graphical topological structures and the conditional independences.</h2><h2 id="induced-markov-network-process"><a href="#induced-markov-network-process" class="headerlink" title="induced markov network process:"></a>induced markov network process:</h2><ol>
<li>找到v-structure，将其父节点之间增加一条边。</li>
<li>三角化，将所有的四边形及其以上边数的多边形，增加尽可能少的边，使其多边形消失。</li>
</ol>
<p><a href="http://wenku.baidu.com/link?url=aYr5IctRG5xGmppznU6BVt8ppFG9lZB4j5jeMddn0mSK6hT5zDe7Z_N2AwG0TbU85fEuXjcrYsD69p6gjWZMC2kgbul8BMzPA9hwrUW1Pfe" target="_blank" rel="external">http://wenku.baidu.com/link?url=aYr5IctRG5xGmppznU6BVt8ppFG9lZB4j5jeMddn0mSK6hT5zDe7Z_N2AwG0TbU85fEuXjcrYsD69p6gjWZMC2kgbul8BMzPA9hwrUW1Pfe</a></p>
<p>[0]<a href="http://blog.csdn.net/thither_shore/article/details/52274903" target="_blank" rel="external">概率图模型(05): 揭示局部概率模型, 稀疏化网络表示(Structured-CPDs)</a><br>[1]<a href="http://www.cse.psu.edu/~rtc12/CSE586/papers/prmlMixturesEM.pdf" target="_blank" rel="external">Mixture Model</a><br>[2]<a href="http://blog.csdn.net/linkin1005/article/details/39054023" target="_blank" rel="external">Blog-斯坦福大学机器学习——高斯判别分析</a><br>[3]<a href="http://faculty.ucmerced.edu/mcarreira-perpinan/papers/phd-ch02.pdf" target="_blank" rel="external">Generative modelling using continuous latent variables</a><br>[4]<a href="https://books.google.co.uk/books?id=7f61BBKdJ4EC&amp;pg=PA373&amp;lpg=PA373&amp;dq=latent+variable+model+independent+parameters&amp;source=bl&amp;ots=PWyCO1KIqp&amp;sig=Nwsh23ffAx1vj-no99Ja_Olbfj0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjEvoiduMXPAhXH1BoKHRuYCVAQ6AEITTAH#v=onepage&amp;q=latent%20variable%20model%20independent%20parameters&amp;f=false" target="_blank" rel="external">learning in graphical models jordan</a></p>

    
  </div>
</article>

</div>


  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">挣点熬夜的咖啡钱</div>
        <ul class="theme.donation.items.length">
        
          <li class="item">
            <img src="/images/wechat_pay.jpg" alt="">
          </li>
        
          <li class="item">
            <img src="/images/alipay.jpg" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>




  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    




  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
