<!DOCTYPE html>


  <html class="light page-post">


<head>
  <meta charset="utf-8">
  
  <title>Expectation Maximum | Liu Xianggen</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="dl,ml,PGM," />
  

  <meta name="description" content="香农信息量、信息熵和交叉熵摘抄自生成对抗网络（GANs）系列文章之一：KL散度和JS散度只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：$$-\log p(x)$$其中对数以2为底，这时香农信息量的单位为比特。香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。如随机事件“中国足球进不了世">
<meta name="keywords" content="dl,ml,PGM">
<meta property="og:type" content="article">
<meta property="og:title" content="Expectation Maximum">
<meta property="og:url" content="http://www.liuxianggen.com/2016/12/20/ml/EM/index.html">
<meta property="og:site_name" content="Liu Xianggen">
<meta property="og:description" content="香农信息量、信息熵和交叉熵摘抄自生成对抗网络（GANs）系列文章之一：KL散度和JS散度只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：$$-\log p(x)$$其中对数以2为底，这时香农信息量的单位为比特。香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。如随机事件“中国足球进不了世">
<meta property="og:updated_time" content="2018-12-29T03:38:10.992Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Expectation Maximum">
<meta name="twitter:description" content="香农信息量、信息熵和交叉熵摘抄自生成对抗网络（GANs）系列文章之一：KL散度和JS散度只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：$$-\log p(x)$$其中对数以2为底，这时香农信息量的单位为比特。香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。如随机事件“中国足球进不了世">

  

  
    <link rel="icon" href="/images/favicon.jpg">
  

  <link href="/css/styles.css?v=028c63b1" rel="stylesheet">


  

  

  <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?fb0e88cc5bbe470f7877739f0bf6bc4c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




</head>

<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#香农信息量、信息熵和交叉熵"><span class="toc-text">香农信息量、信息熵和交叉熵</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-KL散度"><span class="toc-text">2.KL散度</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gaussian-Mixed-Model"><span class="toc-text">Gaussian Mixed Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gaussian-Discrimination-Model"><span class="toc-text">Gaussian Discrimination Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#EM-in-Gaussian-Mixed-Model"><span class="toc-text">EM in Gaussian Mixed Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Jensen’s-inequality"><span class="toc-text">Jensen’s inequality</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p-theta-X"><span class="toc-text">$p(\theta|X)$</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#another-idea"><span class="toc-text">another idea</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-ml/EM" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Expectation Maximum</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2016.12.20</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>LiuXianggen</span>
        </span>
      

      


      

    </div>
  </header>

  <div class="article-content">
    
      <h1 id="香农信息量、信息熵和交叉熵"><a href="#香农信息量、信息熵和交叉熵" class="headerlink" title="香农信息量、信息熵和交叉熵"></a>香农信息量、信息熵和交叉熵</h1><p>摘抄自<a href="https://baijiahao.baidu.com/s?id=1595106694519834247&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">生成对抗网络（GANs）系列文章之一：KL散度和JS散度</a><br>只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：<br>$$<br>-\log p(x)<br>$$<br>其中对数以2为底，这时香农信息量的单位为比特。香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。如随机事件“中国足球进不了世界杯”不需要多少信息量（比如要不要多观察几场球赛的表现）就可以消除不确定性，因此该随机事件的香农信息量就少。再比如“抛一个硬币出现正面”，要消除它的不确定性，通过简单计算，需要1比特信息量，这意味着随机试验完成后才能消除不确定性。</p>
<p>刚才定义了随机变量在一个点处的香农信息量，那么如何衡量随机变量X（或整个样本空间）的总体香农信息量呢？下面就要引出随机变量X的信息熵的概念，或概率分布p的信息熵。信息熵H(p)是香农信息量-logp(x)的数学期望，即所有X=x处的香农信息量的和，由于每一个x的出现概率不一样（用概率密度函数值p(x)衡量），需要用p(x)加权求和。因此信息熵是用于刻画消除随机变量X的不确定性所需要的总体信息量的大小。其数学定义如下：<br>$$<br>H(p) = E_{x\sim p(x)} [-\log p(x)] = -\int_x p(x)\log p(x)<br>$$</p>
<p>交叉熵是什么呢？<br>假设q(x)是用来拟合p(x)的概率分布，x属于p的样本空间，交叉熵用于衡量q在拟合p的过程中，用于消除不确定性而充分使用的信息量大小（理解为衡量q为了拟合p所付出的努力，另外注意交叉熵定义里的“充分使用”和信息熵定义里的“所需”的区别，“充分使用”不一定能达到全部，“所需”是指全部）。由于在每一个点X=x处q的香农信息量为-logq(x)，也就是在点X=x处，q消除不确定性而充分使用的信息量为-logq(x)（理解为衡量q在X=x处为了拟合p所作的努力），那么就可以计算出在整个样本空间上q消除不确定性而充分使用的总体信息量，即-logq(x)的数学期望，由于每个x的权重为p(x)，因此交叉熵H(p,q）为：<br>$$<br>H(p,q) = E_{x\sim p(x)} [-\log q(x)] = -\int_x p(x)\log q(x)<br>$$</p>
<h1 id="2-KL散度"><a href="#2-KL散度" class="headerlink" title="2.KL散度"></a>2.KL散度</h1><p>两个概率分布p和q的KL散度（Kullback–Leibler divergence）也称为相对熵，用于刻画概率分布q拟合概率分布p的程度。在生成对抗网络里，p为真实数据的概率分布，q为随机噪声生成数据的概率分布，对抗的目的是让q充分拟合p。在q拟合概率分布p的过程中，如果q完全拟合p，自然有H(p)=H(p,q)，如果q拟合p不充分，自然产生信息损耗H(p)-H(p,q)，整个信息损耗就是p和q的KL散度。因此p和q的KL散度的定义如下：<br>$$<br>KL(p,q) = H(p)-H(p,q) = E_{p(x)} [-\log p(x)] - E_{p(x)} [-\log q(x)] = \int_x p(x)\log \frac{q(x)}{p(x)    }<br>$$<br>因此散度D(p||q)为信息熵H(p)与交叉熵H(p,q)的差，衡量q拟合p的过程中产生的信息损耗，损耗越少，q拟合p也就越好。很明显散度是不对称的，并不是p和q的距离。</p>
<h1 id="Gaussian-Mixed-Model"><a href="#Gaussian-Mixed-Model" class="headerlink" title="Gaussian Mixed Model"></a>Gaussian Mixed Model</h1><p>There is a latent/hidden/unobserved random variable z.So, $x^i$,$z^i$ have a join a distribution:<br>$$<br>P(x^i,z^i) = P(x^i|z^i)P(z^i)  \\<br>z \sim Multinomal(\phi) \\<br>\phi_j \ge 0 \quad \sum \phi_j=1 \\<br>x^i|z^i=j \sim N(\mu_j,\Sigma_j)<br>$$</p>
<h1 id="Gaussian-Discrimination-Model"><a href="#Gaussian-Discrimination-Model" class="headerlink" title="Gaussian Discrimination Model"></a>Gaussian Discrimination Model</h1><p>预设是：x^i可观测，z^i可观测，直接用Maximum Likelyhood Estimation有：<br>$$<br>l(\phi,\mu,\Sigma) = \sum_{i=1}^M log P(x^i,z^i,\phi,\mu,\Sigma) \\<br>\phi_i = \frac{\sum 1\lbrace z^i=j \rbrace}{M} \\<br>\mu_j = \frac{\sum 1\lbrace z^i=j \rbrace x^i}{\sum 1\lbrace z^i=j \rbrace}<br>$$</p>
<p>但是，通常来说，隐变量z都是不可观测的，所以下面讲解EM算法。</p>
<h1 id="EM-in-Gaussian-Mixed-Model"><a href="#EM-in-Gaussian-Mixed-Model" class="headerlink" title="EM in Gaussian Mixed Model"></a>EM in Gaussian Mixed Model</h1><p>Repeat {<br>E-step (guess value of $z^i$) :<br>    let<br>$$<br>w_j^i = P(z^i=j|x^i,\phi,\mu,\Sigma) \\<br>=\frac{P(x^i|z^i=j)P(z^i=j)}{\sum P(x^i|z^i=l)P(z^i=l)} \\<br>= \frac{ \frac{e^{-\frac{1}{2}(x^i-u_j)^T\Sigma_j^{-1}(x^i-u_j)}}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \phi_j }{\sum_l(\cdots)}<br>$$<br>[注]默认{$z \rightarrow x$}属于高斯混合模型。$w_j^i$可以被理解为第i个x是由第j个高斯分布（z一共有K类，则相当于有K个高斯分布）产生的概率。</p>
<p>M-step:<br>$$<br>\phi_j = \frac{1}{m}\sum_{i=1}^M w_j^i \\<br>\mu_j  = \frac{\sum_{i=1}^M w_j^i x^i}{\sum_{i=1}^M w_j^i} \\<br>\Sigma_j = \frac {\sum_{i=1}^M w_j^i(x^i-\mu_j)(x^i-\mu_j)^T}{\sum_{i=1}^M w_j^i}<br>$$<br>所以第j个高斯分布产生的概率$\phi_j$等于每个样本对z的后验概率的和。</p>
<h1 id="Jensen’s-inequality"><a href="#Jensen’s-inequality" class="headerlink" title="Jensen’s inequality"></a>Jensen’s inequality</h1><p>let f be a convex funtion (e.g. $f’’(x) \ge 0$)<br>Let X be a random variable. Then,<br>$$f(EX)=f(E[X]) \le E[f(x)]<br>$$<br>further, if $f’’(x) \ge 0$,(f is strictly convex,也就是曲线中没有直线), Then<br>$$<br>f(EX)= E[f(x)] \Leftrightarrow X=E[X] \Leftrightarrow \text{概率为1条件下取X}<br>$$<br>In addition, if $f’’le 0$<br>$$<br>f(EX)=f(E[X]) \ge E[f(x)]<br>$$<br>which EM algorithm will use it.</p>
<h1 id="p-theta-X"><a href="#p-theta-X" class="headerlink" title="$p(\theta|X)$"></a>$p(\theta|X)$</h1><p>Have some model for $P(x,z,\theta)$,<br>Observe only x,<br>want to maximize:<br>$$<br>l(\theta) = \sum_{i=1}^M log(P(x^i,\theta)) \\<br>= \sum_{i=1}^M log \sum_{z^i}P(x^i,z^i,\theta) \\<br>\text{So,} \\<br>\max \limits_{\theta}{l(\theta)} \\<br>= \sum_{i=1}^M log \sum_{z^i}P(x^i,z^i,\theta) \\<br>= \sum_{i=1}^M log \sum_{z^i} Q(z^i) \frac{P(x^i,z^i,\theta)}{Q(z^i)} \\<br>= \sum_{i=1}^M log E_{z^i \sim Q}[\frac{P(x^i,z^i,\theta)}{Q(z^i)}]<br>$$<br>where $Q(z^i) \ge 0,\sum_{z \in Val(z)}Q(z)=1$,Q(z)是对P(z)的一个估计。</p>
<p>根据Jenson不等式，log函数是一个凹函数(concave function)，则有$f(EX)=f(E[X]) \ge E[f(x)]$，即，<br>$$\sum_{i=1}^M log E_{z^i \sim Q}[\frac{P(x^i,z^i,\theta)}{Q(z^i)}] \\<br>\ge \sum_{i=1}^M E_{z^i \sim Q} log [\frac{P(x^i,z^i,\theta)}{Q(z^i)}]<br>$$<br>即我们找到了$l(\theta)$的一个下界(lower bound)：<br>$$<br>l(\theta)\ge \sum_{i=1}^M \sum_{z^i} Q(z^i) log [\frac{P(x^i,z^i,\theta)}{Q(z^i)}]<br>$$<br>那么，我们就是要找一个对P的估计Q，让这个lower bound 尽可能的紧。<br>Want<br>$\frac{P(x^i,z^i,\theta)}{Q(z^i)}$ is a consant for all the value of $z^i$(只是说对于相同的$z^i$,值相同，但并不是说就等于常数C),that is,<br>$$<br>Q(z^i) \propto  P(x^i,z^i,\theta)<br>$$<br>又因为$\sum_{z \in Val(z)}Q(z)=1$，所以，<br>$$<br>Q(z^i) = \frac{P(x^i,z^i,\theta)}{\sum_{z^i}P(x^i,z^i,\theta)} \\<br>= \frac{P(x^i,z^i,\theta)}{P(x^i,\theta)}\\<br>= P(z^i|x^i,\theta)<br>$$<br>因此，更为一般性的EM算法可以写为：</p>
<ul>
<li>E-step:<br>set $Q(z^i) = P(z^i|x^i,\theta)$<br>$Q(z^i=j) = w_j$,具体公式上面已经给出。</li>
<li>M-step:<br>$\theta = arg\max \limits_{\theta} \sum_{i=1}^M \sum_{z^i} Q(z^i) log [\frac{P(x^i,z^i,\theta)}{Q(z^i)}]$<br>将E-step结果带入得：<br>$$<br>g = \sum_{i=1}^M \sum_{z^i} Q(z^i) log [\frac{P(x^i,z^i,\theta)}{Q(z^i)}] \\<br>= \sum_{i=1}^M \sum_{z^i} w_j log \frac {\frac{-\frac{1}{2}e^{(x^i-u_j)^T\Sigma_j^{-1}(x^i-u_j)}}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \phi_j}{w_j}<br>$$<br>令$\triangledown g=0$,得出：<br>$$<br>\mu_j  = \frac{\sum_{i=1}^M w_j^i x^i}{\sum_{i=1}^M w_j^i} \\<br>\cdots<br>$$</li>
</ul>
<p>再求$\phi_j$,需要用到其它的优化方法，有约束$\sum_j \phi_j = 1$,我们引入拉格朗日算子，有以下最优化问题：<br>$$<br>\mathcal{L} = \sum_{z^i} w_j log \frac {\frac{-\frac{1}{2}e^{(x^i-u_j)^T\Sigma_j^{-1}(x^i-u_j)}}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \phi_j}{w_j} - \beta(\sum_j \phi_j -1)<br>$$<br>再令：<br>$\frac{\partial \mathcal{L}}{\partial \phi}=0$,求出$\phi$</p>
<h2 id="another-idea"><a href="#another-idea" class="headerlink" title="another idea"></a>another idea</h2><p>define:<br>$$<br>J(\theta,Q) = \sum_{i=1}^M \sum_{z^i} Q(z^i) log [\frac{P(x^i,z^i,\theta)}{Q(z^i)}] \\<br>l(\theta) \ge  J(\theta,Q)<br>$$<br>EM算法是在函数J上的坐标上升算法：<br>E-step: Maximize Q<br>M-step: Maximize $\theta$</p>

    
  </div>
</article>

</div>


  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">挣点熬夜的咖啡钱</div>
        <ul class="theme.donation.items.length">
        
          <li class="item">
            <img src="/images/wechat_pay.jpg" alt="">
          </li>
        
          <li class="item">
            <img src="/images/alipay.jpg" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>




  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    




  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
